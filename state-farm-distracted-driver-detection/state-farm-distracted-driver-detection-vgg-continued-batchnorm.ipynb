{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Farm Distracted Driver Detection - VGG16 Continued\n",
    "\n",
    "This notebook contains the final attempt at the Kaggle State Farm Distracted Driver Detection competition using the VGG model extended with batchnorm. The purpose is to train the best possible model, as well as testing the external vgg16, utils and plot libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Initial Setup\n",
    "\n",
    "Import libraries and functions for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plots displayed inline in notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Make Python 3 consistent\n",
    "from __future__ import print_function, division\n",
    "\n",
    "# Make help libraries available\n",
    "import sys\n",
    "\n",
    "sys.path.append('/home/ubuntu/personal-libraries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "from kerastools.vgg16 import Vgg16\n",
    "from kerastools.utils import get_batches, save_array, load_array, get_classes, do_clip\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, BatchNormalization, Flatten, Dropout\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "## Define model\n",
    "\n",
    "We setup our initial VGG16 model with batchnormalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://files.fast.ai/models/vgg16_bn.h5\n",
      "552861696/553620808 [============================>.] - ETA: 0s_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3, 224, 224)       0         \n",
      "_________________________________________________________________\n",
      "norm_layer (Lambda)          (None, 3, 224, 224)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 3, 226, 226)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_1_0 (Conv2D)      (None, 64, 224, 224)      1792      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 64, 226, 226)      0         \n",
      "_________________________________________________________________\n",
      "conv_layer_1_1 (Conv2D)      (None, 64, 224, 224)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 112, 112)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 64, 114, 114)      0         \n",
      "_________________________________________________________________\n",
      "conv_layer_2_0 (Conv2D)      (None, 128, 112, 112)     73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, 128, 114, 114)     0         \n",
      "_________________________________________________________________\n",
      "conv_layer_2_1 (Conv2D)      (None, 128, 112, 112)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 128, 56, 56)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPaddin (None, 128, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_3_0 (Conv2D)      (None, 256, 56, 56)       295168    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPaddin (None, 256, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_3_1 (Conv2D)      (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPaddin (None, 256, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_3_2 (Conv2D)      (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 256, 28, 28)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPaddin (None, 256, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_4_0 (Conv2D)      (None, 512, 28, 28)       1180160   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPaddin (None, 512, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_4_1 (Conv2D)      (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPaddi (None, 512, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_4_2 (Conv2D)      (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 512, 14, 14)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_5_0 (Conv2D)      (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_5_1 (Conv2D)      (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_13 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_5_2 (Conv2D)      (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 512, 7, 7)         0         \n",
      "_________________________________________________________________\n",
      "flat_layer (Flatten)         (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc_layer_1 (Dense)           (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "fc_layer_2 (Dense)           (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,390,312\n",
      "Trainable params: 138,373,928\n",
      "Non-trainable params: 16,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg = Vgg16(use_batchnorm = True)\n",
    "vgg.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "## Setup batches\n",
    "\n",
    "We define out validation and training badges for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vgg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-e127e1938a66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sample/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mval_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vgg' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "#path = ''\n",
    "path = 'sample/'\n",
    "\n",
    "train_batches = vgg.get_batches(path + 'train', batch_size = batch_size)\n",
    "val_batches = vgg.get_batches(path + 'valid', batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Finetune model - Sample\n",
    "\n",
    "We need to adjust the standard VGG model to our new input with 10 classes, so we finetune it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3, 224, 224)       0         \n",
      "_________________________________________________________________\n",
      "norm_layer (Lambda)          (None, 3, 224, 224)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 3, 226, 226)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_1_0 (Conv2D)      (None, 64, 224, 224)      1792      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 64, 226, 226)      0         \n",
      "_________________________________________________________________\n",
      "conv_layer_1_1 (Conv2D)      (None, 64, 224, 224)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 112, 112)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 64, 114, 114)      0         \n",
      "_________________________________________________________________\n",
      "conv_layer_2_0 (Conv2D)      (None, 128, 112, 112)     73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, 128, 114, 114)     0         \n",
      "_________________________________________________________________\n",
      "conv_layer_2_1 (Conv2D)      (None, 128, 112, 112)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 128, 56, 56)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPaddin (None, 128, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_3_0 (Conv2D)      (None, 256, 56, 56)       295168    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPaddin (None, 256, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_3_1 (Conv2D)      (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPaddin (None, 256, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_3_2 (Conv2D)      (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 256, 28, 28)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPaddin (None, 256, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_4_0 (Conv2D)      (None, 512, 28, 28)       1180160   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPaddin (None, 512, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_4_1 (Conv2D)      (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPaddi (None, 512, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_4_2 (Conv2D)      (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 512, 14, 14)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_5_0 (Conv2D)      (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_5_1 (Conv2D)      (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_13 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_5_2 (Conv2D)      (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 512, 7, 7)         0         \n",
      "_________________________________________________________________\n",
      "flat_layer (Flatten)         (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc_layer_1 (Dense)           (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "fc_layer_2 (Dense)           (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 10)                40970     \n",
      "=================================================================\n",
      "Total params: 134,334,282\n",
      "Trainable params: 40,970\n",
      "Non-trainable params: 134,293,312\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg.finetune(train_batches)\n",
    "vgg.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We train the model using the default learning rate of 0.001 for a single epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "31/31 [==============================] - 26s - loss: 4.9359 - acc: 0.2137 - val_loss: 2.5414 - val_acc: 0.2708\n"
     ]
    }
   ],
   "source": [
    "vgg.fit_batch(train_batches, val_batches, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We see that the accuracy increases fine on the sample, so we increase the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "31/31 [==============================] - 25s - loss: 2.8378 - acc: 0.4265 - val_loss: 2.1342 - val_acc: 0.3824\n",
      "Epoch 2/4\n",
      "31/31 [==============================] - 25s - loss: 2.2904 - acc: 0.5263 - val_loss: 2.1866 - val_acc: 0.3971\n",
      "Epoch 3/4\n",
      "31/31 [==============================] - 25s - loss: 1.9057 - acc: 0.6039 - val_loss: 1.4286 - val_acc: 0.6176\n",
      "Epoch 4/4\n",
      "31/31 [==============================] - 26s - loss: 1.7159 - acc: 0.6210 - val_loss: 1.6112 - val_acc: 0.5521\n"
     ]
    }
   ],
   "source": [
    "vgg.model.optimizer.lr = 0.1\n",
    "\n",
    "vgg.fit_batch(train_batches, val_batches, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Try 4 more epochs with lower learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "31/31 [==============================] - 25s - loss: 1.4559 - acc: 0.6808 - val_loss: 1.2820 - val_acc: 0.6618\n",
      "Epoch 2/4\n",
      "31/31 [==============================] - 25s - loss: 1.4853 - acc: 0.6805 - val_loss: 1.7984 - val_acc: 0.5441\n",
      "Epoch 3/4\n",
      "31/31 [==============================] - 25s - loss: 1.2413 - acc: 0.7311 - val_loss: 1.4632 - val_acc: 0.6176\n",
      "Epoch 4/4\n",
      "31/31 [==============================] - 25s - loss: 1.2351 - acc: 0.7137 - val_loss: 1.6869 - val_acc: 0.5147\n"
     ]
    }
   ],
   "source": [
    "vgg.model.optimizer.lr = 0.001\n",
    "\n",
    "vgg.fit_batch(train_batches, val_batches, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Seems, this is as far as we can get on the sample data set. A pretty good base line in the area of 0.5 - 0.66."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Finetune model - Full data\n",
    "\n",
    "We continue our finetuning on the full data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19624 images belonging to 10 classes.\n",
      "Found 2800 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "path = ''\n",
    "\n",
    "train_batches = vgg.get_batches(path + 'train', batch_size = batch_size)\n",
    "val_batches = vgg.get_batches(path + 'valid', batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We start with a single epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "613/613 [==============================] - 546s - loss: 1.3370 - acc: 0.7291 - val_loss: 2.3063 - val_acc: 0.6103\n"
     ]
    }
   ],
   "source": [
    "vgg.fit_batch(train_batches, val_batches, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We are going much better now. We increase the learning rate and see, where that takes us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "613/613 [==============================] - 545s - loss: 1.0327 - acc: 0.7958 - val_loss: 2.2778 - val_acc: 0.6185\n",
      "Epoch 2/4\n",
      "613/613 [==============================] - 545s - loss: 1.0079 - acc: 0.8136 - val_loss: 2.9188 - val_acc: 0.5629\n",
      "Epoch 3/4\n",
      "613/613 [==============================] - 545s - loss: 1.0285 - acc: 0.8199 - val_loss: 2.9374 - val_acc: 0.5730\n",
      "Epoch 4/4\n",
      "613/613 [==============================] - 546s - loss: 1.0548 - acc: 0.8284 - val_loss: 3.3877 - val_acc: 0.5513\n"
     ]
    }
   ],
   "source": [
    "vgg.model.optimizer.lr = 0.1\n",
    "\n",
    "vgg.fit_batch(train_batches, val_batches, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "And then we lower the learning rate again, and see where we end up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "613/613 [==============================] - 546s - loss: 1.0784 - acc: 0.8345 - val_loss: 3.1749 - val_acc: 0.5928\n",
      "Epoch 2/4\n",
      "613/613 [==============================] - 546s - loss: 1.0959 - acc: 0.8343 - val_loss: 2.9543 - val_acc: 0.6116\n",
      "Epoch 3/4\n",
      "613/613 [==============================] - 545s - loss: 1.1056 - acc: 0.8427 - val_loss: 3.4353 - val_acc: 0.6040\n",
      "Epoch 4/4\n",
      "613/613 [==============================] - 545s - loss: 1.1361 - acc: 0.8414 - val_loss: 3.1795 - val_acc: 0.6048\n"
     ]
    }
   ],
   "source": [
    "vgg.model.optimizer.lr = 0.001\n",
    "\n",
    "vgg.fit_batch(train_batches, val_batches, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The valuation accuracy is pretty good, but we are overfitting quite a lot. Lets try and make more layers trainable and see, if that helps things along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "layers = vgg.model.layers\n",
    "# Get the index of the first dense layer...\n",
    "first_dense_idx = [index for index, layer in enumerate(layers) if type(layer) is Dense][0]\n",
    "# ...and set this and all subsequent layers to trainable\n",
    "for layer in layers[first_dense_idx:]: layer.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "And then we rerun the training. First one epoch with low learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "613/613 [==============================] - 546s - loss: 1.1315 - acc: 0.8445 - val_loss: 3.5571 - val_acc: 0.6019\n"
     ]
    }
   ],
   "source": [
    "vgg.fit_batch(train_batches, val_batches, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then four epochs with a higher learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "613/613 [==============================] - 545s - loss: 1.1549 - acc: 0.8485 - val_loss: 3.2152 - val_acc: 0.6196\n",
      "Epoch 2/4\n",
      "613/613 [==============================] - 545s - loss: 1.2350 - acc: 0.8449 - val_loss: 3.8607 - val_acc: 0.5636\n",
      "Epoch 3/4\n",
      "613/613 [==============================] - 545s - loss: 1.1585 - acc: 0.8545 - val_loss: 3.1586 - val_acc: 0.6445\n",
      "Epoch 4/4\n",
      "613/613 [==============================] - 546s - loss: 1.1995 - acc: 0.8482 - val_loss: 3.3004 - val_acc: 0.6366\n"
     ]
    }
   ],
   "source": [
    "vgg.model.optimizer.lr = 0.1\n",
    "\n",
    "vgg.fit_batch(train_batches, val_batches, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "And then four epochs with a lower learning rate again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "613/613 [==============================] - 546s - loss: 1.1895 - acc: 0.8543 - val_loss: 3.7380 - val_acc: 0.6127\n",
      "Epoch 2/4\n",
      "613/613 [==============================] - 546s - loss: 1.1979 - acc: 0.8517 - val_loss: 3.7769 - val_acc: 0.5838\n",
      "Epoch 3/4\n",
      "613/613 [==============================] - 545s - loss: 1.2458 - acc: 0.8541 - val_loss: 3.5832 - val_acc: 0.6203\n",
      "Epoch 4/4\n",
      "613/613 [==============================] - 545s - loss: 1.2328 - acc: 0.8580 - val_loss: 3.9601 - val_acc: 0.6012\n"
     ]
    }
   ],
   "source": [
    "vgg.model.optimizer.lr = 0.001\n",
    "\n",
    "vgg.fit_batch(train_batches, val_batches, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "That did little to improve things. Let's try an even lower learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "613/613 [==============================] - 546s - loss: 1.2667 - acc: 0.8545 - val_loss: 4.0369 - val_acc: 0.5770\n",
      "Epoch 2/4\n",
      "613/613 [==============================] - 546s - loss: 1.2413 - acc: 0.8590 - val_loss: 3.6576 - val_acc: 0.6109\n",
      "Epoch 3/4\n",
      "613/613 [==============================] - 545s - loss: 1.2565 - acc: 0.8591 - val_loss: 3.8979 - val_acc: 0.6073\n",
      "Epoch 4/4\n",
      "613/613 [==============================] - 545s - loss: 1.3083 - acc: 0.8600 - val_loss: 3.9093 - val_acc: 0.6062\n"
     ]
    }
   ],
   "source": [
    "vgg.model.optimizer.lr = 0.00001\n",
    "\n",
    "vgg.fit_batch(train_batches, val_batches, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We only seem to be stabilizing. Let's save the weights and try a different approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vgg.model.save_weights('models/base_vgg16_norm.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved VGG\n",
    "We continue using the VGG16 network with batchnorm, but attempt to improve it. That is we want to keep the pretrained convolutional layers fixed, and increase our training speed.\n",
    "We start by defining a new VGG16() model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3, 224, 224)       0         \n",
      "_________________________________________________________________\n",
      "norm_layer (Lambda)          (None, 3, 224, 224)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 3, 226, 226)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_1_0 (Conv2D)      (None, 64, 224, 224)      1792      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 64, 226, 226)      0         \n",
      "_________________________________________________________________\n",
      "conv_layer_1_1 (Conv2D)      (None, 64, 224, 224)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 112, 112)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 64, 114, 114)      0         \n",
      "_________________________________________________________________\n",
      "conv_layer_2_0 (Conv2D)      (None, 128, 112, 112)     73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, 128, 114, 114)     0         \n",
      "_________________________________________________________________\n",
      "conv_layer_2_1 (Conv2D)      (None, 128, 112, 112)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 128, 56, 56)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPaddin (None, 128, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_3_0 (Conv2D)      (None, 256, 56, 56)       295168    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPaddin (None, 256, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_3_1 (Conv2D)      (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPaddin (None, 256, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_3_2 (Conv2D)      (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 256, 28, 28)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPaddin (None, 256, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_4_0 (Conv2D)      (None, 512, 28, 28)       1180160   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPaddin (None, 512, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_4_1 (Conv2D)      (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPaddi (None, 512, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_4_2 (Conv2D)      (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 512, 14, 14)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_5_0 (Conv2D)      (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_5_1 (Conv2D)      (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_13 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_5_2 (Conv2D)      (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 512, 7, 7)         0         \n",
      "_________________________________________________________________\n",
      "flat_layer (Flatten)         (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc_layer_1 (Dense)           (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "fc_layer_2 (Dense)           (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,390,312\n",
      "Trainable params: 138,373,928\n",
      "Non-trainable params: 16,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg = Vgg16(use_batchnorm = True)\n",
    "vgg.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then proceed to find the last max pooling layer of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define convolutional layers\n",
    "last_conv_idx = [i for i, l in enumerate(vgg.model.layers) if type(l) is MaxPooling2D][-1]\n",
    "conv_layers = vgg.model.layers[:last_conv_idx + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then define a model using only the convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 3, 224, 224)       0         \n",
      "_________________________________________________________________\n",
      "norm_layer (Lambda)          (None, 3, 224, 224)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 3, 226, 226)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_1_0 (Conv2D)      (None, 64, 224, 224)      1792      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 64, 226, 226)      0         \n",
      "_________________________________________________________________\n",
      "conv_layer_1_1 (Conv2D)      (None, 64, 224, 224)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 112, 112)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 64, 114, 114)      0         \n",
      "_________________________________________________________________\n",
      "conv_layer_2_0 (Conv2D)      (None, 128, 112, 112)     73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, 128, 114, 114)     0         \n",
      "_________________________________________________________________\n",
      "conv_layer_2_1 (Conv2D)      (None, 128, 112, 112)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 128, 56, 56)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPaddin (None, 128, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_3_0 (Conv2D)      (None, 256, 56, 56)       295168    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPaddin (None, 256, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_3_1 (Conv2D)      (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPaddin (None, 256, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_3_2 (Conv2D)      (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 256, 28, 28)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPaddin (None, 256, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_4_0 (Conv2D)      (None, 512, 28, 28)       1180160   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPaddin (None, 512, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_4_1 (Conv2D)      (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPaddi (None, 512, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_4_2 (Conv2D)      (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 512, 14, 14)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_5_0 (Conv2D)      (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_5_1 (Conv2D)      (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_13 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_5_2 (Conv2D)      (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 512, 7, 7)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_model = Sequential(conv_layers)\n",
    "conv_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is now, that we want to pre-computer all of our data through the convolutional layers. This will drastically reduce the training time, once we start experimenting with dense model architecture.\n",
    "We start by defining our batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19624 images belonging to 10 classes.\n",
      "Found 2800 images belonging to 10 classes.\n",
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "path = ''\n",
    "\n",
    "train_batches = get_batches(path + 'train', batch_size = 44, target_size = (224, 224), shuffle = False)\n",
    "\n",
    "valid_batches = get_batches(path + 'valid', batch_size = 50, target_size = (224, 224), shuffle = False)\n",
    "\n",
    "test_batches = get_batches(path + 'test', batch_size = 2, target_size = (224, 224), shuffle = False, class_mode = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also extract labels and classes for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19624 images belonging to 10 classes.\n",
      "Found 2800 images belonging to 10 classes.\n",
      "Found 79726 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "(val_classes, trn_classes, val_labels, trn_labels, \n",
    " val_filenames, filenames, test_filenames) = get_classes(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pre-compute each of our datasets and save the numpy arrays. This eats a lot of memory on the poor AWS instance, so after each data computation, we do some cleanup to realease the memory. We save and load using bcolz, as it utilises great compression and does I/O very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_feat = conv_model.predict_generator(train_batches, np.int(train_batches.samples / train_batches.batch_size))\n",
    "save_array(path + 'results/conv_computed/conv_feat_norm.dat', conv_feat)\n",
    "\n",
    "del conv_feat\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_val_feat = conv_model.predict_generator(valid_batches, np.int(valid_batches.samples / valid_batches.batch_size))\n",
    "save_array(path + 'results/conv_computed/conv_val_feat_norm.dat', conv_val_feat)\n",
    "\n",
    "del conv_val_feat\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_test_feat = conv_model.predict_generator(test_batches, np.int(test_batches.samples / test_batches.batch_size))\n",
    "save_array(path + 'results/conv_computed/conv_test_feat_norm.dat', conv_test_feat)\n",
    "\n",
    "del conv_test_feat\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we can load the three feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv_feat = load_array('results/conv_computed/conv_feat_norm.dat')\n",
    "conv_val_feat = load_array('results/conv_computed/conv_val_feat_norm.dat')\n",
    "conv_test_feat = load_array('results/conv_computed/conv_test_feat_norm.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG model with batchnorm and precomputed augmentation.\n",
    "\n",
    "We precompute some augmented data in order to reduce the overfitting of out model. We start by setting the level of preprocessing and then define a new bunch of batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train the model on the augmented images. First define a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 3, 224, 224)       0         \n",
      "_________________________________________________________________\n",
      "norm_layer (Lambda)          (None, 3, 224, 224)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_40 (ZeroPaddi (None, 3, 226, 226)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_1_0 (Conv2D)      (None, 64, 224, 224)      1792      \n",
      "_________________________________________________________________\n",
      "zero_padding2d_41 (ZeroPaddi (None, 64, 226, 226)      0         \n",
      "_________________________________________________________________\n",
      "conv_layer_1_1 (Conv2D)      (None, 64, 224, 224)      36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 64, 112, 112)      0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_42 (ZeroPaddi (None, 64, 114, 114)      0         \n",
      "_________________________________________________________________\n",
      "conv_layer_2_0 (Conv2D)      (None, 128, 112, 112)     73856     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_43 (ZeroPaddi (None, 128, 114, 114)     0         \n",
      "_________________________________________________________________\n",
      "conv_layer_2_1 (Conv2D)      (None, 128, 112, 112)     147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 128, 56, 56)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_44 (ZeroPaddi (None, 128, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_3_0 (Conv2D)      (None, 256, 56, 56)       295168    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_45 (ZeroPaddi (None, 256, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_3_1 (Conv2D)      (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "zero_padding2d_46 (ZeroPaddi (None, 256, 58, 58)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_3_2 (Conv2D)      (None, 256, 56, 56)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 256, 28, 28)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_47 (ZeroPaddi (None, 256, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_4_0 (Conv2D)      (None, 512, 28, 28)       1180160   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_48 (ZeroPaddi (None, 512, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_4_1 (Conv2D)      (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_49 (ZeroPaddi (None, 512, 30, 30)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_4_2 (Conv2D)      (None, 512, 28, 28)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 512, 14, 14)       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_50 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_5_0 (Conv2D)      (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_51 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_5_1 (Conv2D)      (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "zero_padding2d_52 (ZeroPaddi (None, 512, 16, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv_layer_5_2 (Conv2D)      (None, 512, 14, 14)       2359808   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 512, 7, 7)         0         \n",
      "_________________________________________________________________\n",
      "flat_layer (Flatten)         (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc_layer_1 (Dense)           (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "fc_layer_2 (Dense)           (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "prediction (Dense)           (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,390,312\n",
      "Trainable params: 138,373,928\n",
      "Non-trainable params: 16,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19624 images belonging to 10 classes.\n",
      "Found 2800 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "gen_t = image.ImageDataGenerator(rotation_range = 15,\n",
    "                                 height_shift_range = 0.05,\n",
    "                                 shear_range = 0.1,\n",
    "                                 channel_shift_range = 20,\n",
    "                                 width_shift_range = 0.1)\n",
    "path = ''\n",
    "\n",
    "da_batches = vgg.get_batches(path + 'train',\n",
    "                             gen_t,\n",
    "                             batch_size = 44,\n",
    "                             shuffle = True,\n",
    "                             target_size = (224, 224))\n",
    "val_batches = vgg.get_batches(path + 'valid', batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetune and make all dense layers trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg.finetune(da_batches)\n",
    "\n",
    "layers = vgg.model.layers\n",
    "# Get the index of the first dense layer...\n",
    "first_dense_idx = [index for index, layer in enumerate(layers) if type(layer) is Dense][0]\n",
    "# ...and set this and all subsequent layers to trainable\n",
    "for layer in layers[first_dense_idx:]: layer.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then train the model using the augmented batches. First run some epochs at default learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "446/446 [==============================] - 545s - loss: 3.0292 - acc: 0.4316 - val_loss: 1.5342 - val_acc: 0.5453\n",
      "Epoch 2/4\n",
      "446/446 [==============================] - 544s - loss: 2.2398 - acc: 0.5530 - val_loss: 1.6148 - val_acc: 0.5495\n",
      "Epoch 3/4\n",
      "446/446 [==============================] - 544s - loss: 2.1429 - acc: 0.5811 - val_loss: 1.6315 - val_acc: 0.5766\n",
      "Epoch 4/4\n",
      "446/446 [==============================] - 544s - loss: 2.1489 - acc: 0.5919 - val_loss: 1.6767 - val_acc: 0.6051\n"
     ]
    }
   ],
   "source": [
    "vgg.fit_batch(da_batches, val_batches, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then increase the learning rate and run some more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      " 40/446 [=>............................] - ETA: 440s - loss: 2.3440 - acc: 0.5835"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-74d96c92bcec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mda_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/ubuntu/personal-libraries/kerastools/imagerecon.pyc\u001b[0m in \u001b[0;36mfit_batch\u001b[0;34m(self, batches, val_batches, epochs)\u001b[0m\n\u001b[1;32m    148\u001b[0m                                  \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                                  \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_batches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                                  validation_steps = np.int(val_batches.samples / val_batches.batch_size))\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[1;32m   1838\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1839\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1840\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1563\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/keras/backend/theano_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/ifelse.pyc\u001b[0m in \u001b[0;36mthunk\u001b[0;34m()\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mthunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vgg.model.optimizer.lr = 0.01\n",
    "\n",
    "vgg.fit_batch(da_batches, val_batches, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally decrease the learning rate and run some more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg.model.optimizer.lr = 0.001\n",
    "\n",
    "vgg.fit_batch(train_batches, val_batches, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudolabeling\n",
    "\n",
    "We're going to try using a combination of [pseudo labeling](http://deeplearning.net/wp-content/uploads/2013/03/pseudo_label_final.pdf) and [knowledge distillation](https://arxiv.org/abs/1503.02531) to allow us to use unlabeled data (i.e. do semi-supervised learning). For our initial experiment we'll use the validation set as the unlabeled data, so that we can see that it is working without using the test set. Afterwards we add the test set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_pseudo = vgg.predict(val_batches, batch_size = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We concatenate thse pseudo labels with our training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comb_pseudo = np.concatenate([da_trn_labels, val_pseudo])\n",
    "comb_feat = np.concatenate([da_conv_feat, conv_val_feat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train our model using the extended data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120544 samples, validate on 2800 samples\n",
      "Epoch 1/1\n",
      "120544/120544 [==============================] - 49s - loss: 0.3544 - acc: 0.8902 - val_loss: 0.4987 - val_acc: 0.8382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1f34107ed0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model_bigger.optimizer.lr = 0.001\n",
    "\n",
    "bn_model_bigger.fit(x = comb_feat,\n",
    "                    y = comb_pseudo,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs = 1,\n",
    "                    validation_data = (conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not really see much of an improvement. Let's try 4 more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120544 samples, validate on 2800 samples\n",
      "Epoch 1/4\n",
      "120544/120544 [==============================] - 50s - loss: 0.3426 - acc: 0.8951 - val_loss: 0.4796 - val_acc: 0.8421\n",
      "Epoch 2/4\n",
      "120544/120544 [==============================] - 50s - loss: 0.3360 - acc: 0.8967 - val_loss: 0.5171 - val_acc: 0.8375\n",
      "Epoch 3/4\n",
      "120544/120544 [==============================] - 50s - loss: 0.3269 - acc: 0.8995 - val_loss: 0.5132 - val_acc: 0.8371\n",
      "Epoch 4/4\n",
      "120544/120544 [==============================] - 50s - loss: 0.3224 - acc: 0.9010 - val_loss: 0.5321 - val_acc: 0.8364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1f348ef5d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model_bigger.fit(x = comb_feat,\n",
    "                    y = comb_pseudo,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs = 4,\n",
    "                    validation_data = (conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are crossing the 0.9 threshold of accuracy. Lets lower the learning rate and train for 4 more epochs and see where that gets us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 120544 samples, validate on 2800 samples\n",
      "Epoch 1/4\n",
      "120544/120544 [==============================] - 50s - loss: 0.3134 - acc: 0.9038 - val_loss: 0.5191 - val_acc: 0.8411\n",
      "Epoch 2/4\n",
      "120544/120544 [==============================] - 50s - loss: 0.3062 - acc: 0.9060 - val_loss: 0.5285 - val_acc: 0.8414\n",
      "Epoch 3/4\n",
      "120544/120544 [==============================] - 50s - loss: 0.3033 - acc: 0.9071 - val_loss: 0.4933 - val_acc: 0.8443\n",
      "Epoch 4/4\n",
      "120544/120544 [==============================] - 50s - loss: 0.2933 - acc: 0.9103 - val_loss: 0.4806 - val_acc: 0.8475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1f34887f10>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model_bigger.optimizer.lr = 0.00001\n",
    "\n",
    "bn_model_bigger.fit(x = comb_feat,\n",
    "                    y = comb_pseudo,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs = 4,\n",
    "                    validation_data = (conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we do see a pretty nice improvement. Enough to warrent us trying with the entire test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pseudo = bn_model_bigger.predict(conv_test_feat, batch_size = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We concatenate thse pseudo labels with our training and valuation pseudo labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comb_pseudo = np.concatenate([comb_pseudo, test_pseudo])\n",
    "comb_feat = np.concatenate([comb_feat, conv_test_feat])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train our model using the extended data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200270 samples, validate on 2800 samples\n",
      "Epoch 1/1\n",
      "200270/200270 [==============================] - 84s - loss: 0.5545 - acc: 0.8444 - val_loss: 0.4587 - val_acc: 0.8454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1f34183290>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model_bigger.optimizer.lr = 0.001\n",
    "\n",
    "bn_model_bigger.fit(x = comb_feat,\n",
    "                    y = comb_pseudo,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs = 1,\n",
    "                    validation_data = (conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hrm, too early to say, but valuation accuracy holds still. Let's run 4 more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200270 samples, validate on 2800 samples\n",
      "Epoch 1/4\n",
      "200270/200270 [==============================] - 84s - loss: 0.5158 - acc: 0.8610 - val_loss: 0.4763 - val_acc: 0.8346\n",
      "Epoch 2/4\n",
      "200270/200270 [==============================] - 83s - loss: 0.4985 - acc: 0.8692 - val_loss: 0.4769 - val_acc: 0.8339\n",
      "Epoch 3/4\n",
      "200270/200270 [==============================] - 83s - loss: 0.4936 - acc: 0.8704 - val_loss: 0.4463 - val_acc: 0.8493\n",
      "Epoch 4/4\n",
      "200270/200270 [==============================] - 84s - loss: 0.4850 - acc: 0.8746 - val_loss: 0.4865 - val_acc: 0.8396\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1f58620f50>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model_bigger.fit(x = comb_feat,\n",
    "                    y = comb_pseudo,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs = 4,\n",
    "                    validation_data = (conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's lower the learning rate again and run 4 more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200270 samples, validate on 2800 samples\n",
      "Epoch 1/4\n",
      "200270/200270 [==============================] - 83s - loss: 0.4803 - acc: 0.8757 - val_loss: 0.4721 - val_acc: 0.8389\n",
      "Epoch 2/4\n",
      "200270/200270 [==============================] - 84s - loss: 0.4764 - acc: 0.8771 - val_loss: 0.4542 - val_acc: 0.8464\n",
      "Epoch 3/4\n",
      "200270/200270 [==============================] - 83s - loss: 0.4750 - acc: 0.8788 - val_loss: 0.4759 - val_acc: 0.8396\n",
      "Epoch 4/4\n",
      "200270/200270 [==============================] - 83s - loss: 0.4700 - acc: 0.8799 - val_loss: 0.4844 - val_acc: 0.8364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1f58650fd0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model_bigger.optimizer.lr = 0.00001\n",
    "\n",
    "bn_model_bigger.fit(x = comb_feat,\n",
    "                    y = comb_pseudo,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs = 4,\n",
    "                    validation_data = (conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are improving slowly. Let's run for 10 more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200270 samples, validate on 2800 samples\n",
      "Epoch 1/10\n",
      "200270/200270 [==============================] - 83s - loss: 0.4671 - acc: 0.8815 - val_loss: 0.4400 - val_acc: 0.8461\n",
      "Epoch 2/10\n",
      "200270/200270 [==============================] - 83s - loss: 0.4642 - acc: 0.8820 - val_loss: 0.5003 - val_acc: 0.8371\n",
      "Epoch 3/10\n",
      "200270/200270 [==============================] - 83s - loss: 0.4617 - acc: 0.8831 - val_loss: 0.5024 - val_acc: 0.8386\n",
      "Epoch 4/10\n",
      "200270/200270 [==============================] - 84s - loss: 0.4578 - acc: 0.8849 - val_loss: 0.5042 - val_acc: 0.8339\n",
      "Epoch 5/10\n",
      "200270/200270 [==============================] - 83s - loss: 0.4557 - acc: 0.8858 - val_loss: 0.4949 - val_acc: 0.8368\n",
      "Epoch 6/10\n",
      "200270/200270 [==============================] - 83s - loss: 0.4552 - acc: 0.8843 - val_loss: 0.4806 - val_acc: 0.8379\n",
      "Epoch 7/10\n",
      "200270/200270 [==============================] - 83s - loss: 0.4532 - acc: 0.8865 - val_loss: 0.4609 - val_acc: 0.8436\n",
      "Epoch 8/10\n",
      "200270/200270 [==============================] - 83s - loss: 0.4512 - acc: 0.8877 - val_loss: 0.4960 - val_acc: 0.8354\n",
      "Epoch 9/10\n",
      "200270/200270 [==============================] - 83s - loss: 0.4471 - acc: 0.8880 - val_loss: 0.4750 - val_acc: 0.8425\n",
      "Epoch 10/10\n",
      "200270/200270 [==============================] - 83s - loss: 0.4461 - acc: 0.8886 - val_loss: 0.4816 - val_acc: 0.8364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1f348ef8d0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_model_bigger.fit(x = comb_feat,\n",
    "                    y = comb_pseudo,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs = 10,\n",
    "                    validation_data = (conv_val_feat, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having trained the model, we save the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bn_model_bigger.save_weights('models/batchnorm_vgg16.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Submitting to Kaggle\n",
    "\n",
    "We finally submit the improved model to Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We start by finding the optimal level of clipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2800 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "valid_batches_pred = get_batches(path + 'valid', batch_size = 50, target_size = (224, 224), shuffle = False, class_mode = None)\n",
    "conv_val_feat_pred = conv_model.predict_generator(valid_batches_pred, np.int(valid_batches_pred.samples / valid_batches_pred.batch_size))\n",
    "\n",
    "val_predictions = bn_model_bigger.predict(conv_val_feat_pred, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def do_clip(arr, mx): return np.clip(arr, (1 - mx) / 9, mx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We then proceed to determine the optimal level of clipping using the validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0000000000000002, 0.48164355679327142]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_clip = []\n",
    "for i in np.arange(0.70, 1.0, 0.01):\n",
    "    test_clip.append([i, categorical_crossentropy(val_labels, do_clip(val_predictions, i)).eval().mean()])\n",
    "\n",
    "min(test_clip, key = lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here it is said, that no clipping is best. Weird. Lets submit two. One clipped and one not clipped. First we compute the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_predictions = bn_model_bigger.predict(conv_test_feat, batch_size = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Define classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "classes = sorted(valid_batches.class_indices, key = valid_batches.class_indices.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then we make a cliping based on earlier experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sumbit_pred = do_clip(test_predictions, 0.89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Then we prepare a submission without clipping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>c0</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>c6</th>\n",
       "      <th>c7</th>\n",
       "      <th>c8</th>\n",
       "      <th>c9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_81601.jpg</td>\n",
       "      <td>0.046099</td>\n",
       "      <td>0.006723</td>\n",
       "      <td>0.004307</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>0.001594</td>\n",
       "      <td>0.001567</td>\n",
       "      <td>0.026646</td>\n",
       "      <td>0.020867</td>\n",
       "      <td>0.016716</td>\n",
       "      <td>0.874798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_14887.jpg</td>\n",
       "      <td>0.689640</td>\n",
       "      <td>0.007839</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>0.003189</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.005292</td>\n",
       "      <td>0.287327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_62885.jpg</td>\n",
       "      <td>0.005301</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>0.017661</td>\n",
       "      <td>0.973055</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.001571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_45125.jpg</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>0.007311</td>\n",
       "      <td>0.017353</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>0.002328</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.656182</td>\n",
       "      <td>0.006239</td>\n",
       "      <td>0.302687</td>\n",
       "      <td>0.004926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_22633.jpg</td>\n",
       "      <td>0.138601</td>\n",
       "      <td>0.055678</td>\n",
       "      <td>0.011123</td>\n",
       "      <td>0.001665</td>\n",
       "      <td>0.004929</td>\n",
       "      <td>0.012565</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>0.007558</td>\n",
       "      <td>0.186375</td>\n",
       "      <td>0.558814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             img        c0        c1        c2        c3        c4        c5  \\\n",
       "0  img_81601.jpg  0.046099  0.006723  0.004307  0.000682  0.001594  0.001567   \n",
       "1  img_14887.jpg  0.689640  0.007839  0.000621  0.002267  0.001608  0.003189   \n",
       "2  img_62885.jpg  0.005301  0.000154  0.000257  0.017661  0.973055  0.000493   \n",
       "3  img_45125.jpg  0.002003  0.007311  0.017353  0.000462  0.002328  0.000509   \n",
       "4  img_22633.jpg  0.138601  0.055678  0.011123  0.001665  0.004929  0.012565   \n",
       "\n",
       "         c6        c7        c8        c9  \n",
       "0  0.026646  0.020867  0.016716  0.874798  \n",
       "1  0.001825  0.000393  0.005292  0.287327  \n",
       "2  0.000514  0.000016  0.000978  0.001571  \n",
       "3  0.656182  0.006239  0.302687  0.004926  \n",
       "4  0.022692  0.007558  0.186375  0.558814  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_no_clip = pd.DataFrame(test_predictions, columns = classes)\n",
    "submission_no_clip.insert(0, 'img', [a[8:] for a in test_batches.filenames])\n",
    "submission_no_clip.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "And a submission with clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img</th>\n",
       "      <th>c0</th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>c6</th>\n",
       "      <th>c7</th>\n",
       "      <th>c8</th>\n",
       "      <th>c9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>img_81601.jpg</td>\n",
       "      <td>0.046099</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.026646</td>\n",
       "      <td>0.020867</td>\n",
       "      <td>0.016716</td>\n",
       "      <td>0.874798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>img_14887.jpg</td>\n",
       "      <td>0.689640</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.287327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>img_62885.jpg</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.017661</td>\n",
       "      <td>0.890000</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.012222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>img_45125.jpg</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.017353</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.656182</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.302687</td>\n",
       "      <td>0.012222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>img_22633.jpg</td>\n",
       "      <td>0.138601</td>\n",
       "      <td>0.055678</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.012565</td>\n",
       "      <td>0.022692</td>\n",
       "      <td>0.012222</td>\n",
       "      <td>0.186375</td>\n",
       "      <td>0.558814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             img        c0        c1        c2        c3        c4        c5  \\\n",
       "0  img_81601.jpg  0.046099  0.012222  0.012222  0.012222  0.012222  0.012222   \n",
       "1  img_14887.jpg  0.689640  0.012222  0.012222  0.012222  0.012222  0.012222   \n",
       "2  img_62885.jpg  0.012222  0.012222  0.012222  0.017661  0.890000  0.012222   \n",
       "3  img_45125.jpg  0.012222  0.012222  0.017353  0.012222  0.012222  0.012222   \n",
       "4  img_22633.jpg  0.138601  0.055678  0.012222  0.012222  0.012222  0.012565   \n",
       "\n",
       "         c6        c7        c8        c9  \n",
       "0  0.026646  0.020867  0.016716  0.874798  \n",
       "1  0.012222  0.012222  0.012222  0.287327  \n",
       "2  0.012222  0.012222  0.012222  0.012222  \n",
       "3  0.656182  0.012222  0.302687  0.012222  \n",
       "4  0.022692  0.012222  0.186375  0.558814  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_clip = pd.DataFrame(sumbit_pred, columns = classes)\n",
    "submission_clip.insert(0, 'img', [a[8:] for a in test_batches.filenames])\n",
    "submission_clip.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally we save the two submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "submission_file_name_no_clip = 'results/augmented-pseudo-vgg-no-clip.gz'\n",
    "submission_no_clip.to_csv(submission_file_name_no_clip, index = False, compression = 'gzip')\n",
    "\n",
    "submission_file_name_clip = 'results/augmented-pseudo-vgg-clip.gz'\n",
    "submission_clip.to_csv(submission_file_name_clip, index = False, compression = 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='results/augmented-pseudo-vgg-no-clip.gz' target='_blank'>results/augmented-pseudo-vgg-no-clip.gz</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/kaggle/state-farm-distracted-driver-detection/results/augmented-pseudo-vgg-no-clip.gz"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink('results/augmented-pseudo-vgg-no-clip.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='results/augmented-pseudo-vgg-clip.gz' target='_blank'>results/augmented-pseudo-vgg-clip.gz</a><br>"
      ],
      "text/plain": [
       "/home/ubuntu/kaggle/state-farm-distracted-driver-detection/results/augmented-pseudo-vgg-clip.gz"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink('results/augmented-pseudo-vgg-clip.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "source": [
    "Turns out that in this case, the no clipping submission actually performed best, by a absolute 0.03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
